{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRk2oV9LzvGC16mXssYopd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RolakeOkans/JO-neuron-connectivity-clustering/blob/main/jo_connectivity_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtLtX88dBM9G"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# #DOWNSTREAM CLUSTERING: JO-A & JO-B  NEURONS\n",
        "# ============================================\n",
        "\n",
        "# ---- Install dependencies (Colab) ----\n",
        "!pip -q install scikit-learn yellowbrick fafbseg\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "from scipy.spatial.distance import squareform\n",
        "\n",
        "# Optional: only import flywire if you truly use it (comment out if unused)\n",
        "# from fafbseg import flywire\n",
        "\n",
        "# --------------------------------------------\n",
        "# Config (set these to your private file paths)\n",
        "# --------------------------------------------\n",
        "INPUT_SYNAPSE_CSV = \"PATH/TO/synapse_table.csv\"\n",
        "POST_CELL_MAP_CSV = \"PATH/TO/post_cell_type_mapping.csv\"\n",
        "NBLAST_SCORES_CSV = \"PATH/TO/nblast_scores.csv\"\n",
        "\n",
        "OUTDIR = \"outputs\"          # keep outputs local; do NOT commit\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# Turn plotting OFF for public version by default (prevents generating result images)\n",
        "ENABLE_PLOTS = False\n",
        "\n",
        "# --------------------------------------------\n",
        "# Helper utilities\n",
        "# --------------------------------------------\n",
        "def safe_save_csv(df: pd.DataFrame, filename: str) -> None:\n",
        "    \"\"\"Save intermediate tables locally. Consider excluding OUTDIR via .gitignore.\"\"\"\n",
        "    path = os.path.join(OUTDIR, filename)\n",
        "    df.to_csv(path, index=False)\n",
        "\n",
        "def safe_save_fig(filename: str) -> None:\n",
        "    \"\"\"Save figures locally only when ENABLE_PLOTS=True.\"\"\"\n",
        "    if not ENABLE_PLOTS:\n",
        "        return\n",
        "    path = os.path.join(OUTDIR, filename)\n",
        "    plt.savefig(path, dpi=300, bbox_inches=\"tight\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# 1) Load + filter synapse table\n",
        "# --------------------------------------------\n",
        "def load_and_filter_synapses(input_csv: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Expected columns include:\n",
        "      - cell_type (str)\n",
        "      - syn_count (numeric)\n",
        "      - post_root_id (int/str)\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(input_csv)\n",
        "\n",
        "    # Filter out right hemisphere labels like *_R<number> and syn_count < 5\n",
        "    df_left = df[\n",
        "        ~df[\"cell_type\"].astype(str).str.contains(r\"_R\\d+$\", regex=True) &\n",
        "        (pd.to_numeric(df[\"syn_count\"], errors=\"coerce\") >= 5)\n",
        "    ].copy()\n",
        "\n",
        "    return df_left\n",
        "\n",
        "# --------------------------------------------\n",
        "# 2) Add post_cell_type labels via mapping file\n",
        "# --------------------------------------------\n",
        "def add_post_cell_type(df_left: pd.DataFrame, mapping_csv: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Mapping file expected columns include:\n",
        "      - post_root_id (or post_pt_root_id)\n",
        "      - post_cell_type\n",
        "    \"\"\"\n",
        "    post_map = pd.read_csv(mapping_csv).copy()\n",
        "\n",
        "    # Harmonize column name if needed\n",
        "    if \"post_pt_root_id\" in post_map.columns and \"post_root_id\" not in post_map.columns:\n",
        "        post_map = post_map.rename(columns={\"post_pt_root_id\": \"post_root_id\"})\n",
        "\n",
        "    # Deduplicate mapping\n",
        "    post_map = post_map.drop_duplicates(subset=\"post_root_id\")\n",
        "\n",
        "    id_to_celltype = dict(zip(post_map[\"post_root_id\"], post_map[\"post_cell_type\"]))\n",
        "    df_left[\"post_cell_type\"] = df_left[\"post_root_id\"].map(id_to_celltype)\n",
        "\n",
        "    return df_left\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3) Connectivity pivot table (for clustering)\n",
        "# --------------------------------------------\n",
        "def make_connectivity_pivot(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    grouped = df.groupby([\"cell_type\", \"post_root_id\"], as_index=False)[\"syn_count\"].sum()\n",
        "    pivot = grouped.pivot(index=\"cell_type\", columns=\"post_root_id\", values=\"syn_count\").fillna(0)\n",
        "\n",
        "    # Replace values of 4 or less with 0 (thresholding)\n",
        "    pivot = pivot.where(pivot > 4, 0)\n",
        "\n",
        "    return pivot\n",
        "\n",
        "# --------------------------------------------\n",
        "# 4) Hierarchical clustering (connectivity)\n",
        "# --------------------------------------------\n",
        "def hierarchical_clustering(matrix: np.ndarray, labels, method: str = \"ward\"):\n",
        "    scaler = Normalizer(norm=\"l2\")\n",
        "    normed = scaler.fit_transform(matrix)\n",
        "    link = linkage(normed, method=method)\n",
        "\n",
        "    if ENABLE_PLOTS:\n",
        "        plt.figure(figsize=(24, 12))\n",
        "        dendrogram(link, labels=labels, leaf_rotation=90, leaf_font_size=8)\n",
        "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
        "        plt.xlabel(\"Pre Cell Type\")\n",
        "        plt.ylabel(\"Distance\")\n",
        "        plt.tight_layout()\n",
        "        safe_save_fig(\"connectivity_dendrogram.png\")\n",
        "        plt.show()\n",
        "\n",
        "    return link\n",
        "\n",
        "# --------------------------------------------\n",
        "# 5) Clustermap prep (optional + no display)\n",
        "# --------------------------------------------\n",
        "def make_log_heatmap(df_with_types: pd.DataFrame, auditory_post_cell_types: list[str]) -> pd.DataFrame:\n",
        "    heatmap_data = (\n",
        "        df_with_types.groupby([\"cell_type\", \"post_cell_type\"])[\"syn_count\"]\n",
        "        .sum()\n",
        "        .reset_index()\n",
        "    )\n",
        "    heatmap_matrix = heatmap_data.pivot(index=\"cell_type\", columns=\"post_cell_type\", values=\"syn_count\")\n",
        "\n",
        "    auditory_sorted = sorted(auditory_post_cell_types)\n",
        "    other_sorted = sorted([c for c in heatmap_matrix.columns if c not in auditory_sorted])\n",
        "    column_order = auditory_sorted + other_sorted\n",
        "    heatmap_matrix = heatmap_matrix.reindex(columns=column_order)\n",
        "\n",
        "    log_heatmap = np.log1p(heatmap_matrix)\n",
        "    return log_heatmap\n",
        "\n",
        "# --------------------------------------------\n",
        "# 6) NBLAST cleaning + scaling\n",
        "# --------------------------------------------\n",
        "def load_clean_nblast(nblast_csv: str) -> pd.DataFrame:\n",
        "    nblast = pd.read_csv(nblast_csv, index_col=0)\n",
        "\n",
        "    pattern = r\"_R\\d+$\"\n",
        "    nblast = nblast[~nblast.index.astype(str).str.contains(pattern, regex=True)]\n",
        "    nblast = nblast.loc[:, ~nblast.columns.astype(str).str.contains(pattern, regex=True)]\n",
        "\n",
        "    # Min-max scale similarity matrix to [0, 1]\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_values = scaler.fit_transform(nblast.values)\n",
        "    df_scaled = pd.DataFrame(scaled_values, index=nblast.index, columns=nblast.columns)\n",
        "\n",
        "    return df_scaled\n",
        "\n",
        "# --------------------------------------------\n",
        "# 7) Combine connectivity similarity + NBLAST\n",
        "# --------------------------------------------\n",
        "def combine_similarity(conn_pivot: pd.DataFrame, nblast_scaled: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Cosine similarity over connectivity features\n",
        "    conn_sim = cosine_similarity(conn_pivot.values)\n",
        "    conn_sim_df = pd.DataFrame(conn_sim, index=conn_pivot.index, columns=conn_pivot.index)\n",
        "\n",
        "    common = conn_sim_df.index.intersection(nblast_scaled.index)\n",
        "\n",
        "    conn_df = conn_sim_df.loc[common, common]\n",
        "    nblast_df = nblast_scaled.loc[common, common]\n",
        "\n",
        "    combined = conn_df + nblast_df\n",
        "\n",
        "    # Min-max normalize combined similarity\n",
        "    min_val = combined.min().min()\n",
        "    max_val = combined.max().max()\n",
        "    norm_sim = (combined - min_val) / (max_val - min_val)\n",
        "\n",
        "    # Convert similarity -> distance\n",
        "    dist = 1 - norm_sim\n",
        "    np.fill_diagonal(dist.values, 0)\n",
        "\n",
        "    # Force symmetry\n",
        "    sym = (dist.values + dist.values.T) / 2\n",
        "    dist_sym = pd.DataFrame(sym, index=dist.index, columns=dist.columns)\n",
        "    np.fill_diagonal(dist_sym.values, 0)\n",
        "\n",
        "    return dist_sym\n",
        "\n",
        "# --------------------------------------------\n",
        "# 8) Cluster combined distance (optional plots)\n",
        "# --------------------------------------------\n",
        "def cluster_distance(distance_df_sym: pd.DataFrame, threshold: float = 1.8):\n",
        "    condensed = squareform(distance_df_sym.values)\n",
        "    link = linkage(condensed, method=\"ward\")\n",
        "\n",
        "    if ENABLE_PLOTS:\n",
        "        plt.figure(figsize=(24, 12))\n",
        "        dendrogram(link, labels=distance_df_sym.index.tolist(), leaf_rotation=90, leaf_font_size=8,\n",
        "                   color_threshold=threshold)\n",
        "        plt.title(f\"Combined Clustering (Threshold = {threshold})\")\n",
        "        plt.xlabel(\"Neuron\")\n",
        "        plt.ylabel(\"Distance\")\n",
        "        plt.tight_layout()\n",
        "        safe_save_fig(\"combined_dendrogram.png\")\n",
        "        plt.show()\n",
        "\n",
        "    clusters = fcluster(link, t=threshold, criterion=\"distance\")\n",
        "    cluster_series = pd.Series(clusters, index=distance_df_sym.index, name=\"cluster\")\n",
        "\n",
        "    return link, cluster_series\n",
        "\n",
        "# --------------------------------------------\n",
        "# MAIN PIPELINE (code-only; no outputs shown)\n",
        "# --------------------------------------------\n",
        "def main():\n",
        "    # 1) Load + filter synapses\n",
        "    df_left = load_and_filter_synapses(INPUT_SYNAPSE_CSV)\n",
        "\n",
        "    # 2) Add post_cell_type\n",
        "    df_left = add_post_cell_type(df_left, POST_CELL_MAP_CSV)\n",
        "\n",
        "    # 3) Connectivity pivot\n",
        "    conn_pivot = make_connectivity_pivot(df_left)\n",
        "\n",
        "    # 4) Hierarchical clustering on connectivity\n",
        "    _ = hierarchical_clustering(conn_pivot.to_numpy(), labels=conn_pivot.index.tolist())\n",
        "\n",
        "    # 5) Optional heatmap prep (no plotting by default)\n",
        "    auditory_post_cell_types = [\n",
        "        # Keep list if it’s not sensitive; otherwise move to a private config\n",
        "        \"A1\", \"A2\", \"AVLP_pr01-1\", \"AVLP_pr23\", \"aPN2\", \"B1-1\", \"B1-2\", \"B1-3\", \"B1-4\", \"B1-5\",\n",
        "        \"B1-6\", \"B1-7\", \"B1-u\", \"B2\", \"GF\", \"IPS_pr01-2\", \"IPS_pr02\", \"JO-A\", \"JO-B\",\n",
        "        \"SAD_pr01\", \"SAD_pr02\", \"vpoEN\", \"WED-VLP-1\", \"WED-VLP-2\", \"WED_pr02\",\n",
        "        \"WV-WV-1\", \"WV-WV-2\", \"WV-WV-3\", \"WV-WV-4\"\n",
        "    ]\n",
        "    log_heatmap = make_log_heatmap(df_left, auditory_post_cell_types)\n",
        "\n",
        "    # 6) Load + clean NBLAST, scale\n",
        "    nblast_scaled = load_clean_nblast(NBLAST_SCORES_CSV)\n",
        "\n",
        "    # 7) Combine similarity matrices -> distance\n",
        "    dist_sym = combine_similarity(conn_pivot, nblast_scaled)\n",
        "\n",
        "    # 8) Cluster combined distance\n",
        "    _, cluster_series = cluster_distance(dist_sym, threshold=1.8)\n",
        "\n",
        "    # Save cluster assignments locally (optional; do NOT commit)\n",
        "    if ENABLE_PLOTS:\n",
        "        safe_save_csv(cluster_series.reset_index().rename(columns={\"index\": \"neuron\"}), \"cluster_assignments.csv\")\n",
        "\n",
        "    # IMPORTANT: No prints, no head(), no shapes in public notebook\n",
        "    return\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "#UPSTREAM CLUSTERING: JO-A & JO-B  NEURONS\n",
        "# ============================================\n",
        "\n",
        "!pip -q install scikit-learn yellowbrick fafbseg\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
        "\n",
        "# --------------------------------------------\n",
        "# Config (replace with your PRIVATE file paths)\n",
        "# --------------------------------------------\n",
        "CODEX_CONNECTIONS_CSV = \"PATH/TO/connections_princeton.csv\"\n",
        "JO_CLUSTER_LIST_CSV   = \"PATH/TO/JO_cluster_list_with_pre_root_id_upstream.csv\"  # has post_root_id + JO neuron name\n",
        "PRE_CELL_MAP_CSV      = \"PATH/TO/JO_Upstream_cell_types_mapping.csv\"             # pre_root_id -> pre_cell_type\n",
        "CLUSTER_ASSIGN_CSV    = \"PATH/TO/JO_left_cluster_list_ordered_by_dendrogram.csv\" # optional, for cluster color bar\n",
        "\n",
        "OUTDIR = \"outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "ENABLE_PLOTS = False  # OFF by default for public version\n",
        "\n",
        "# --------------------------------------------\n",
        "# Helper utilities\n",
        "# --------------------------------------------\n",
        "def save_csv_local(df: pd.DataFrame, filename: str) -> None:\n",
        "    df.to_csv(os.path.join(OUTDIR, filename), index=False)\n",
        "\n",
        "def save_fig_local(filename: str) -> None:\n",
        "    if not ENABLE_PLOTS:\n",
        "        return\n",
        "    plt.savefig(os.path.join(OUTDIR, filename), dpi=300, bbox_inches=\"tight\")\n",
        "\n",
        "# --------------------------------------------\n",
        "# 1) Build upstream table: find all pre neurons connecting to JO (as posts)\n",
        "# --------------------------------------------\n",
        "def build_upstream_table(connections_csv: str, jo_csv: str) -> pd.DataFrame:\n",
        "    big_df = pd.read_csv(connections_csv)\n",
        "    jo_df = pd.read_csv(jo_csv)\n",
        "\n",
        "    # Normalize naming: ensure JO neuron name column becomes 'cell_type'\n",
        "    if \"JO_neuron\" in jo_df.columns and \"cell_type\" not in jo_df.columns:\n",
        "        jo_df = jo_df.rename(columns={\"JO_neuron\": \"cell_type\"})\n",
        "\n",
        "    # Filter connections to only JO posts (upstream means pre_root_id -> post_root_id (JO))\n",
        "    filtered = big_df[big_df[\"post_root_id\"].isin(jo_df[\"post_root_id\"])].copy()\n",
        "\n",
        "    # Merge JO neuron names\n",
        "    merged = filtered.merge(jo_df[[\"post_root_id\", \"cell_type\"]], on=\"post_root_id\", how=\"left\")\n",
        "\n",
        "    # Reorder columns (optional)\n",
        "    cols = [\"cell_type\"] + [c for c in merged.columns if c != \"cell_type\"]\n",
        "    merged = merged[cols]\n",
        "\n",
        "    return merged\n",
        "\n",
        "# --------------------------------------------\n",
        "# 2) Pivot upstream connectivity for clustering\n",
        "# --------------------------------------------\n",
        "def make_upstream_pivot(up_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    grouped = (\n",
        "        up_df.groupby([\"cell_type\", \"pre_root_id\"], as_index=False)[\"syn_count\"]\n",
        "        .sum()\n",
        "    )\n",
        "\n",
        "    pivot = grouped.pivot(index=\"cell_type\", columns=\"pre_root_id\", values=\"syn_count\").fillna(0)\n",
        "\n",
        "    # Threshold small counts\n",
        "    pivot = pivot.where(pivot > 4, 0)\n",
        "    return pivot\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3) Hierarchical clustering (upstream)\n",
        "# --------------------------------------------\n",
        "def cluster_upstream(pivot: pd.DataFrame, method: str = \"ward\"):\n",
        "    data_matrix = pivot.to_numpy()\n",
        "\n",
        "    scaler = Normalizer(norm=\"l2\")\n",
        "    normed = scaler.fit_transform(data_matrix)\n",
        "\n",
        "    link = linkage(normed, method=method)\n",
        "\n",
        "    if ENABLE_PLOTS:\n",
        "        plt.figure(figsize=(24, 12))\n",
        "        dendrogram(link, labels=pivot.index.tolist(), leaf_rotation=90, leaf_font_size=8)\n",
        "        plt.title(\"Hierarchical Clustering Dendrogram — JO Upstream\")\n",
        "        plt.xlabel(\"Post Cell Type (JO)\")\n",
        "        plt.ylabel(\"Distance\")\n",
        "        plt.tight_layout()\n",
        "        save_fig_local(\"JO_upstream_dendrogram.png\")\n",
        "        plt.show()\n",
        "\n",
        "    return link\n",
        "\n",
        "# --------------------------------------------\n",
        "# 4) Add pre_cell_type labels + create heatmap matrix\n",
        "# --------------------------------------------\n",
        "def add_pre_cell_type(up_df: pd.DataFrame, pre_map_csv: str) -> pd.DataFrame:\n",
        "    pre_map = pd.read_csv(pre_map_csv)\n",
        "\n",
        "    # Normalize naming\n",
        "    if \"pre_pt_root_id\" in pre_map.columns and \"pre_root_id\" not in pre_map.columns:\n",
        "        pre_map = pre_map.rename(columns={\"pre_pt_root_id\": \"pre_root_id\"})\n",
        "\n",
        "    # Filter syn_count >= 5 (as in your original)\n",
        "    up_df2 = up_df.copy()\n",
        "    up_df2[\"syn_count\"] = pd.to_numeric(up_df2[\"syn_count\"], errors=\"coerce\")\n",
        "    up_df2 = up_df2[up_df2[\"syn_count\"] >= 5]\n",
        "\n",
        "    merged = up_df2.merge(pre_map, on=\"pre_root_id\", how=\"left\")  # adds pre_cell_type\n",
        "    return merged\n",
        "\n",
        "def make_pretype_heatmap_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    heatmap_df = df.pivot_table(\n",
        "        index=\"pre_cell_type\",\n",
        "        columns=\"cell_type\",\n",
        "        values=\"syn_count\",\n",
        "        aggfunc=\"sum\"\n",
        "    )\n",
        "    return heatmap_df\n",
        "\n",
        "# --------------------------------------------\n",
        "# 5) Apply manual ordering + log transform (no printing)\n",
        "# --------------------------------------------\n",
        "def reorder_and_log_transform(heatmap_df: pd.DataFrame,\n",
        "                              priority_types: list[str],\n",
        "                              jo_neuron_order: list[str]) -> pd.DataFrame:\n",
        "    # Row order: priority first, then remaining alphabetical\n",
        "    priority_sorted = sorted(priority_types)\n",
        "    remaining = sorted([x for x in heatmap_df.index if x not in priority_sorted])\n",
        "    final_row_order = priority_sorted + remaining\n",
        "    heatmap_df = heatmap_df.reindex(index=final_row_order)\n",
        "\n",
        "    # Ensure all JO neurons exist as columns (fill missing with NaN)\n",
        "    for neuron in jo_neuron_order:\n",
        "        if neuron not in heatmap_df.columns:\n",
        "            heatmap_df[neuron] = np.nan\n",
        "\n",
        "    final_col_order = [c for c in jo_neuron_order if c in heatmap_df.columns]\n",
        "    heatmap_df = heatmap_df.reindex(columns=final_col_order)\n",
        "\n",
        "    # Log-transform: use log10, handle 0 -> -inf -> NaN\n",
        "    log_df = np.log10(heatmap_df)\n",
        "    log_df.replace(-np.inf, np.nan, inplace=True)\n",
        "\n",
        "    return log_df, final_row_order, final_col_order\n",
        "\n",
        "# --------------------------------------------\n",
        "# 6) Optional plotting (disabled by default)\n",
        "# --------------------------------------------\n",
        "def plot_upstream_heatmap(log_df: pd.DataFrame, outname: str):\n",
        "    if not ENABLE_PLOTS:\n",
        "        return\n",
        "\n",
        "    # compute vmin/vmax ignoring NaNs\n",
        "    non_nan = log_df.values[np.isfinite(log_df.values)]\n",
        "    if non_nan.size == 0:\n",
        "        return\n",
        "    vmin, vmax = float(np.min(non_nan)), float(np.max(non_nan))\n",
        "\n",
        "    cmap = plt.get_cmap(\"viridis_r\").copy()\n",
        "    cmap.set_bad(color=\"#d0d0d0\")\n",
        "\n",
        "    plt.figure(figsize=(24, 18))\n",
        "    ax = sns.heatmap(\n",
        "        log_df,\n",
        "        mask=log_df.isna(),\n",
        "        cmap=cmap,\n",
        "        vmin=vmin,\n",
        "        vmax=vmax,\n",
        "        linewidths=0.25,\n",
        "        linecolor=\"black\",\n",
        "        xticklabels=True,\n",
        "        yticklabels=True,\n",
        "        cbar_kws={\"label\": \"Synapse count (log10)\"}\n",
        "    )\n",
        "    ax.yaxis.tick_right()\n",
        "    ax.yaxis.set_label_position(\"right\")\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=6)\n",
        "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=6)\n",
        "    plt.tight_layout()\n",
        "    save_fig_local(outname)\n",
        "    plt.show()\n",
        "\n",
        "# --------------------------------------------\n",
        "# MAIN (code-only)\n",
        "# --------------------------------------------\n",
        "def main():\n",
        "    # Build upstream table\n",
        "    upstream = build_upstream_table(CODEX_CONNECTIONS_CSV, JO_CLUSTER_LIST_CSV)\n",
        "    save_csv_local(upstream, \"filtered_JO_left_pre_ids_CODEONLY.csv\")  # local only\n",
        "\n",
        "    # Pivot + cluster\n",
        "    pivot = make_upstream_pivot(upstream)\n",
        "    _ = cluster_upstream(pivot)\n",
        "\n",
        "    # Add pre_cell_type mapping\n",
        "    upstream_pretyped = add_pre_cell_type(upstream, PRE_CELL_MAP_CSV)\n",
        "    save_csv_local(upstream_pretyped, \"upstream_with_pre_cell_type_CODEONLY.csv\")\n",
        "\n",
        "    # Heatmap matrix\n",
        "    heatmap_df = make_pretype_heatmap_matrix(upstream_pretyped)\n",
        "\n",
        "    # Manual ordering lists (keep here, or move to a private config)\n",
        "    priority_types = [\n",
        "        \"AVLP_pr01-1\", \"B2\", \"IPS_pr02\", \"JO-A\", \"JO-B\", \"JO-EDM\", \"JO-DP\", \"JO-mz\",\n",
        "        \"SAD_pr01\", \"SAD_pr02\", \"WED_pr02\", \"WV-WV-3\",\n",
        "    ]\n",
        "\n",
        "    jo_neuron_order = [\n",
        "        # Keep your full list here (truncated for brevity in this template)\n",
        "        # Paste your whole jo_neuron_order list from your notebook\n",
        "        \"JO-A_L16\", \"JO-A_L21\", \"JO-A_L31\", \"JO-A_L1\",\n",
        "        \"JO-B_L124\", \"JO-B_L40\", \"JO-B_L132\", \"JO-B_L145\",\n",
        "        # ...\n",
        "    ]\n",
        "\n",
        "    log_df, final_row_order, final_col_order = reorder_and_log_transform(\n",
        "        heatmap_df, priority_types, jo_neuron_order\n",
        "    )\n",
        "\n",
        "    # Optional plot (OFF by default)\n",
        "    plot_upstream_heatmap(log_df, \"JO_upstream_heatmap_CODEONLY.png\")\n",
        "\n",
        "    # Optional: export the matrix used for plotting (local only; do NOT commit)\n",
        "    # To keep extra safe, save a *structure-only* version (no values) by default:\n",
        "    structure_only = pd.DataFrame(index=log_df.index, columns=log_df.columns)\n",
        "    structure_only.to_csv(os.path.join(OUTDIR, \"JO_upstream_heatmap_structure_ONLY.csv\"))\n",
        "\n",
        "    return\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "fcefz31gB8Ev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}